**zookeeper** 
    
分布式锁基本原理：

            结合前面对 zookeeper 特性的分析和理解，我们可以利用 zookeeper 节点的特性来实现独占锁，就是同级节点
        的唯一性，多个进程往 zookeeper 的指定节点下创建一个相同名称的节点，只有一个能成功，另外一个是创建失败；
        创建失败的节点全部通过 zookeeper 的 watcher 机制来监听 zookeeper 这个子节点的变化，一旦监听到子节点的删
        除事件，则再次触发所有进程去写锁。这种实现方式很简单，但是会产生“惊群效应”，
        
        利用有序节点来实现分布式锁:
         
            我们可以通过有序节点来实现分布式锁，每个客户端都往指定的节点下注册一个临时有序节点，越早创建的节点，节点的顺序编号就越小，
        那么我们可以判断子节点中最的节点设置为获得锁。如果自己的节点不是所有子节点中最小的，意味着还没有获得锁。这个的实现和前面单节点实现的差异性在于，
        每个节点只需要监听比自己小的节点，当比自己小的节点删除以后，客户端会收到 watcher 事件，此时再次判断自己的节点是不是所有子节点中最小的，
        如果是则获得锁，否则就不断重复这个过程，这样就不会导致羊群效应，因为每个客户端只需要监控一个节点。
        
    一般使用curator实现分布式锁：
        curator 对于锁这块做了一些封装，curator 提供了 InterProcessMutex 这样一个 api。除了分布式锁之外，还提供了 leader 选举、分布式队列等常用的功能。
        InterProcessMutex：分布式可重入排它锁
        InterProcessSemaphoreMutex：分布式排它锁
        InterProcessReadWriteLock：分布式读写锁    
         
    ZAB协议
        崩溃恢复 模式         
        原子广播 模式
            当整个集群在启动时，或者当 leader 节点出现网络中断、崩溃等情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader，当 leader 服务器选举出来后，
        并且集群中有过半的机器和该 leader 节点完成数据同步后（同步指的是数据同步，用来保证集群中过半的机器能够和 leader 服务器的数据状态保持一致），
        ZAB 协议就会退出恢复模式。
            当集群中已经有过半的 Follower 节点完成了和 Leader 状态同步以后，那么整个集群就进入了消息广播模式。这个时候，
        在 Leader 节点正常工作时，启动一台新的服务器加入到集群，那这个服务器会直接进入数据恢复模式，和leader 节点进行数据同步。
        同步完成后即可正常对外提供非事务请求的处理。
        
        leader 节点可以处理事务请求和非事务请求，follower 节点只能处理非事务请求，如果 follower 节点接收到非事务请求，会把这个请求转发给 Leader 服务器
            
         ps:(原子广播原理) 
            和完整的 2pc 事务不一样的地方在于，zab 协议不能终止事务，follower 节点要么 ACK 给 leader，要么抛弃
         leader，只需要保证过半数的节点响应这个消息并提交了即可，虽然在某一个时刻 follower 节点和 leader 节点的
         状态会不一致，但是也是这个特性提升了集群的整体性能。 当然这种数据不一致的问题，zab 协议提供了一种恢复模式来进行数据恢复
         ps:(崩溃恢复原理) 
            1 已经被处理的消息不能丢
            2 被丢弃的消息不能再次出现
            如果在2PC的commit阶段leader崩掉了，且没有想follower发送commit 命令。这个时候新的leader产生之后，原先的leader也重新加入
         变成follower，并且它保留上一个朝代的最新事务id，但是没有用，因为epoch 老了，且这条消息会被丢弃！
            
                 
zookeeper 的数据同步问题

    类2PC 过半2pc
    <详细需要再总结>

zookeeper 的顺序一致性
    
    <详细需要再总结>
    
**zookeeper 选举原理**

    myid  服务器id
    zxid  事务id
    epoch 逻辑时钟id
    
    选举状态  
    LOOKING，竞选状态。
    FOLLOWING，随从状态，同步 leader 状态，参与投票。
    OBSERVING，观察状态,同步 leader 状态，不参与投票。
    LEADING，领导者状态。
    
服务器启动时的 leader 选举
    
    server1 server2 server3
    每个节点启动的时候状态都是 LOOKING，处于观望状态，接下来就开始进行选主流程：
    1   每个 Server 发出一个投票。由于是初始情况，Server1 和 Server2 都会将自己作为 Leader 服务器来进行投票，每次投票会包含所推举的
        服务器的 myid 和 ZXID、epoch，使用(myid, ZXID,epoch)来表示，此时 Server1 的投票为(1,0,1)，Server2 的投票为(2,0,1)，Server3 的投票为(3,0,1)
        然后各自将这个投票发给集群中其他机器。
    2   接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票（epoch）、是否来自LOOKING 状态的服务器。
    3   处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行 PK，PK 规则如下:
        1.  优先比较 epoch
        2.  其次检查 ZXID。ZXID 比较大的服务器优先作为 Leader
        3.  如果 ZXID 相同，那么就比较 myid。myid 较大的服务器作为Leader 服务器。
    4   统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于 Server1、Server2 ，server3 而言，都
        统计出集群中已经有两台机器接受了server3 的投票信息，此时便认为已经选出了 Leader。如果没有，则用PK胜出的myid exid epoch来发送下一次投票
    5   改变服务器状态。一旦确定了 Leader，每个服务器就会更新自己的状态，如果是 Follower，那么就变更为 FOLLOWING，如果是 Leader，就变更为 LEADING。
    选举完成！
    
    源码分析流程：
        QuorumPeerMain——解析zoo.cfg——创建并且启动定时数据清理——判断集群单机——创建通信——从快照文件中恢复数据——启动NIO2181 ——开始发起选举socket——开始选举流程——zab算法——完成选举
        
运行过程中的 leader 选举
    
    当集群中的 leader 服务器出现宕机或者不可用的情况时，那么整个集群将无法对外提供服务，而是进入新一轮的 Leader 选举，服务器运行期间的 Leader 选举和启动时期的 Leader 选举基本过程是一致的。    
    
    变更状态。Leader 挂后，余下的非 Observer 服务器都会将自己的服务器状态变更为 LOOKING，然后开始进入 Leader 选举过程。
    1   2   3   4   5 同启动时是一样的 
    注意：
        如果在commit阶段leader崩掉了，且没有想follower发送commit 命令。这个时候新的leader产生之后，原先的leader也重新加入变成follower，
    并且它保留上一个朝代的最新事务id，但是没有用，因为epoch 老了，且这条消息会被丢弃！    
    
    
    
    
    
    